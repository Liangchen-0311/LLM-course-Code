{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6535b606-4b8a-4635-8b90-aca49f5cf51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.1515, -0.5459, -0.8552,  0.2976,  0.0460,  0.0296,  1.4862,  0.9704],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([ 2.1515, -0.5459, -0.8552,  0.2976,  0.0460,  0.0296,  1.4862,  0.9704],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "====================================================================================================\n",
      "tensor([0.0806, 0.0000, 0.0000, 0.2075, 0.0000, 0.3678, 0.0000, 0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.0821, 0.0438, 0.0000, 0.2411, 0.0000, 0.3375, 0.0000, 0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n",
      "tensor([0.3642, 0.0816, 0.0000, 0.2423, 0.0000, 0.3399, 0.0000, 0.0000],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义多头自注意力类，继承自nn.Module\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, hidden_size, num_header, masked=False):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        # 确保hidden_size可以被num_header整除\n",
    "        assert hidden_size % num_header == 0, f\"header的数目没办法整除:{hidden_size}, {num_header}\"\n",
    "        \n",
    "        self.hidden_size = hidden_size  # 输入向量的维度\n",
    "        self.num_header = num_header    # 注意力头的数量\n",
    "        \n",
    "        # 定义WQ、WK、WV，它们都是将输入向量映射到不同空间的线性层\n",
    "        self.wq = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        )\n",
    "        self.wk = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        )\n",
    "        self.wv = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size)\n",
    "        )\n",
    "        # 定义WO，用于在最后将多头的输出合并回原始维度\n",
    "        self.wo = nn.Sequential(\n",
    "            nn.Linear(in_features=self.hidden_size, out_features=self.hidden_size),\n",
    "            nn.ReLU()  # 激活函数\n",
    "        )\n",
    "        self.masked = masked  # 是否使用掩码\n",
    "\n",
    "    # 分割函数，将Q、K、V的输出分割成多个头\n",
    "    def split(self, vs):\n",
    "        n, t, e = vs.shape  # n: batch size, t: sequence length, e: feature dimension\n",
    "        # 将vs重塑并交换维度，以适应多头机制\n",
    "        vs = torch.reshape(vs, shape=(n, t, self.num_header, e // self.num_header))\n",
    "        vs = torch.permute(vs, dims=(0, 2, 1, 3))\n",
    "        return vs\n",
    "\n",
    "    # 前向传播函数\n",
    "    def forward(self, x):\n",
    "        # 获取Q、K、V\n",
    "        q = self.wq(x)  # 输入x经过WQ线性变换\n",
    "        k = self.wk(x)  # 输入x经过WK线性变换\n",
    "        v = self.wv(x)  # 输入x经过WV线性变换\n",
    "        \n",
    "        # 分割Q、K、V为多个头\n",
    "        q = self.split(q)\n",
    "        k = self.split(k)\n",
    "        v = self.split(v)\n",
    "        \n",
    "        # 计算Q和K的相关性，得到注意力权重\n",
    "        # torch.permute重新排列多维张量维度的函数。\n",
    "        scores = torch.matmul(q, torch.permute(k, dims=(0, 1, 3, 2)))  # scores.shape == [n, h, t, t]\n",
    "        \n",
    "        # 如果使用掩码，则在上三角部分加上一个很大的负数，使得上三角的权重在Softmax后接近0\n",
    "        if self.masked:\n",
    "            mask = torch.ones((t, t))\n",
    "            mask = torch.triu(mask, diagonal=1) * -10000  # 上三角为-10000\n",
    "            mask = mask[None][None]  # 广播到[n, h, t, t]\n",
    "            scores = scores + mask\n",
    "        \n",
    "        # 将分数转换为权重，使用Softmax函数\n",
    "        alpha = torch.softmax(scores, dim=-1)  # alpha.shape == [n, h, t, t]\n",
    "        \n",
    "        # 使用权重和V相乘，得到加权的V\n",
    "        v = torch.matmul(alpha, v)  # v.shape == [n, h, t, v]\n",
    "        \n",
    "        # 将多头的输出合并回原始维度\n",
    "        v = torch.permute(v, dims=(0, 2, 1, 3))  # 交换维度\n",
    "        n, t, _, _ = v.shape  # 重新计算维度\n",
    "        v = torch.reshape(v, shape=(n, t, -1))  # 重塑为原始维度\n",
    "        v = self.wo(v)  # 经过WO线性变换和ReLU激活函数\n",
    "        return v\n",
    "\n",
    "# 测试函数\n",
    "def t0():\n",
    "    # 创建一个简单的输入张量token_id\n",
    "    token_id = torch.tensor([\n",
    "        [1, 3, 5],\n",
    "        [1, 6, 3],\n",
    "        [2, 3, 1],\n",
    "        [5, 1, 2],\n",
    "        [6, 1, 2]\n",
    "    ])\n",
    "    \n",
    "    # 创建一个简单的Embedding层\n",
    "    emb_layer = nn.Embedding(num_embeddings=10, embedding_dim=8)\n",
    "    x1 = emb_layer(token_id)  # 将token_id嵌入到8维空间\n",
    "    \n",
    "    # 打印一些输出\n",
    "    print(x1[0][0])  # 第一个样本的第一个token的嵌入向量\n",
    "    print(x1[1][0])  # 第二个样本的第一个token的嵌入向量\n",
    "    print(\"=\" * 100)\n",
    "    \n",
    "    # 创建多头自注意力实例\n",
    "    att = MultiHeadSelfAttention(hidden_size=8, num_header=2)\n",
    "    # 将嵌入向量通过多头自注意力层\n",
    "    x3 = att(x1)\n",
    "    # 打印一些输出\n",
    "    print(x3[0][0])  # 第一个样本的第一个token经过自注意力后的向量\n",
    "    print(x3[1][0])  # 第二个样本的第一个token经过自注意力后的向量\n",
    "    print(x3[1][1])  # 第二个样本的第二个token经过自注意力后的向量\n",
    "\n",
    "# 程序入口\n",
    "if __name__ == '__main__':\n",
    "    t0()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58b619-5b2b-4657-9575-a4e5c919be1b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
